@article{Cho2016,
   abstract = {In this paper, we consider the problem of (multiple) changepoint detection in panel data. We propose the double CUSUM statistic which utilises the cross-sectional change-point structure by examining the cumulative sums of ordered CUSUMs at each point. The efficiency of the proposed change-point test is studied, which is reflected on the rate at which the cross-sectional size of a change is permitted to converge to zero while it is still detectable. Also, the consistency of the proposed changepoint detection procedure based on the binary segmentation algorithm, is established in terms of both the total number and locations (in time) of the estimated change-points. Motivated by the representation properties of the Generalised Dynamic Factor Model, we propose a bootstrap procedure for test criterion selection, which accounts for both cross-sectional and within-series correlations in high-dimensional data. The empirical performance of the double CUSUM statistics, equipped with the proposed bootstrap scheme, is investigated in a comparative simulation study with the state-of-the-art. As an application, we analyse the log returns of S&P 100 component stock prices over a period of one year.},
   author = {Haeran Cho},
   doi = {10.1214/16-EJS1155},
   issn = {19357524},
   issue = {2},
   journal = {Electronic Journal of Statistics},
   keywords = {Binary segmentation,CUSUM statistics,Change-point analysis,High-dimensional data analysis},
   pages = {2000-2038},
   publisher = {Institute of Mathematical Statistics},
   title = {Change-point detection in panel data via double CUSUM statistic},
   volume = {10},
   year = {2016},
}
@misc{Cho2015,
   author = {Haeran Cho and Piotr Fryzlewicz},
   isbn = {202414:21:41},
   issue = {2},
   journal = {Source: Journal of the Royal Statistical Society. Series B (Statistical Methodology},
   pages = {475-507},
   title = {Multiple-change-point detection for high dimensional time series via sparsified binary segmentation},
   volume = {77},
   url = {https://www.jstor.org/stable/24774746},
   year = {2015},
}
@misc{Bai1998,
   author = {Jushan Bai and Pierre Perron},
   issue = {1},
   pages = {47-78},
   title = {Estimating and Testing Linear Models with Multiple Structural Changes},
   volume = {66},
   year = {1998},
}
@article{Li2012,
   abstract = {This paper studies genetic algorithms as ameans of estimating the number of changepoints and their locations in a climatic time series. Such methods bypass classic subsegmentation algorithms, which sometimes yield suboptimal conclusions. Minimum description length techniques are introduced. These techniques require optimizing an objective function over all possible changepoint numbers and location times. The general objective functions allow for correlated data, reference station aspects, and/or nonnormal marginal distributions, all common features of climate time series. As an exhaustive evaluation of all changepoint configurations is not possible, the optimization is accomplished via a genetic algorithm that randomly walks through a subset of good models in an intelligent manner. The methods are applied in the analysis of 173 yr of annual precipitation measurements from New Bedford, Massachusetts, and the North Atlantic basin tropical cyclone record. © 2012 American Meteorological Society.},
   author = {Shanghong Li and Robert Lund},
   doi = {10.1175/2011JCLI4055.1},
   issn = {08948755},
   issue = {2},
   journal = {Journal of Climate},
   keywords = {Algorithms,North Atlantic Ocean,Precipitation,Time series,Tropical cyclones},
   month = {1},
   pages = {674-686},
   title = {Multiple changepoint detection via genetic algorithms},
   volume = {25},
   year = {2012},
}
@article{Killick2012,
   abstract = {In this article, we consider the problem of detecting multiple changepoints in large datasets. Our focus is on applications where the number of changepoints will increase as we collect more data: For example, in genetics as we analyze larger regions of the genome, or in finance as we observe time series over longer periods. We consider the common approach of detecting changepoints through minimizing a cost function over possible numbers and locations of changepoints. This includes several established procedures for detecting changing points, such as penalized likelihood and minimum description length. We introduce a new method for finding the minimum of such cost functions and hence the optimal number and location of changepoints that has a computational cost, which, under mild conditions, is linear in the number of observations. This compares favorably with existing methods for the same problem whose computational cost can be quadratic or even cubic. In simulation studies, we show that our new method can be orders of magnitude faster than these alternative exact methods. We also compare with the binary segmentation algorithm for identifying changepoints, showing that the exactness of our approach can lead to substantial improvements in the accuracy of the inferred segmentation of the data. This article has supplementary materials available online. © 2012 American Statistical Association.},
   author = {R. Killick and P. Fearnhead and I. A. Eckley},
   doi = {10.1080/01621459.2012.737745},
   issn = {01621459},
   issue = {500},
   journal = {Journal of the American Statistical Association},
   keywords = {Dynamic programming,PELT,Segmentation,Structural change},
   pages = {1590-1598},
   title = {Optimal detection of changepoints with a linear computational cost},
   volume = {107},
   year = {2012},
}
@article{Yann2015,
   abstract = {This paper addresses the retrospective or off-line multiple change-point detection problem. In this context, there is a need of efficient diagnostic tools that enable to localize the segmentation uncertainty along the observed sequence. Concerning the segmentation uncertainty, the focus was mainly on the change-point position uncertainty. We propose to state this problem in a new way, viewing multiple change-point models as latent structure models and using results from information theory. This led us to show that the segmentation uncertainty is not reflected in the posterior distributions of the change-point position because of the marginalization that is intrinsic in the computation of these posterior distributions. The entropy of the segmentation of a given observed sequence can be considered as the canonical measure of segmentation uncertainty. This segmentation entropy can be decomposed as conditional entropy profiles that enables to localize this canonical segmentation uncertainty along the sequence. One of the main outcomes of this work is to derive efficient algorithms to compute these conditional entropy profiles. The proposed approach benefits from all the properties of the Shannon–Khinchin axioms of entropy and therefore is the unique approach for localizing the canonical segmentation uncertainty along the sequence. We introduce the Kullback–Leibler divergence of the uniform distribution from the segmentation distribution for successive numbers of change points as a new tool for assessing the number of change points selected by different methods. The proposed approach is illustrated using four contrasted examples.},
   author = {Yann Guédon},
   doi = {10.1007/s11222-013-9433-1},
   issn = {15731375},
   issue = {2},
   journal = {Statistics and Computing},
   keywords = {Entropy,Kullback–Leibler divergence,Latent structure model,Multiple change-point detection,Smoothing algorithm},
   month = {3},
   pages = {303-320},
   publisher = {Kluwer Academic Publishers},
   title = {Segmentation uncertainty in multiple change-point models},
   volume = {25},
   year = {2015},
}
@article{Aminikhanghahi2017,
   author = {Samaneh Aminikhanghahi and Diane J. Cook},
   doi = {10.1007/s10115-016-0987-z},
   issn = {0219-1377},
   issue = {2},
   journal = {Knowledge and Information Systems},
   month = {5},
   pages = {339-367},
   title = {A survey of methods for time series change point detection},
   volume = {51},
   url = {http://link.springer.com/10.1007/s10115-016-0987-z},
   year = {2017},
}

@online{baumer2025,
	author = {Baumer, Benjamin S. and Suárez Sierra, Biviana Marcela},
	title = {Tidychangepoint: A Unified Framework for Analyzing
	Changepoint Detection in Univariate Time Series},
	date = {2025-07-09},
	langid = {en},
	abstract = {We present {[}tidychangepoint{]}\{.pkg\}, a new
	{[}R{]}\{.proglang\} package for changepoint detection analysis.
	Most {[}R{]}\{.proglang\} packages for segmenting univariate time
	series focus on providing one or two algorithms for changepoint
	detection that work with a small set of models and penalized
	objective functions, and all of them return a custom, nonstandard
	object type. This makes comparing results across various algorithms,
	models, and penalized objective functions unnecessarily difficult.
	{[}tidychangepoint{]}\{.pkg\} solves this problem by wrapping
	functions from a variety of existing packages and storing the
	results in a common S3 class called `tidycpt`. The package then
	provides functionality for easily extracting comparable numeric or
	graphical information from a `tidycpt` object, all in a
	{[}tidyverse{]}\{.pkg\}-compliant framework.
	{[}tidychangepoint{]}\{.pkg\} is versatile: it supports both
	deterministic algorithms like PELT (from {[}changepoint{]}\{.pkg\}),
	and also flexible, randomized, genetic algorithms (via
	{[}GA{]}\{.pkg\}) that-\/-\/-via new functionality built into
	{[}tidychangepoint{]}\{.pkg\}-\/-\/-can be used with any compliant
	model-fitting function and any penalized objective function. By
	bringing all of these disparate tools together in a cohesive
	fashion, {[}tidychangepoint{]}\{.pkg\} facilitates comparative
	analysis of changepoint detection algorithms and models.}
}

@article{Killick01122012,
	author = {R. Killick and P. Fearnhead and I. A. Eckley},
	title = {Optimal Detection of Changepoints With a Linear Computational Cost},
	journal = {Journal of the American Statistical Association},
	volume = {107},
	number = {500},
	pages = {1590--1598},
	year = {2012},
	publisher = {ASA Website},
	doi = {10.1080/01621459.2012.737745},
	
	
	URL = { 
	
	https://doi.org/10.1080/01621459.2012.737745
	
	
	
	},
	eprint = { 
	
	https://doi.org/10.1080/01621459.2012.737745
	
	
	
	}
	
}

@article {Shi2022,
	author = "Xueheng Shi and Claudie Beaulieu and Rebecca Killick and Robert Lund",
	title = "Changepoint Detection: An Analysis of the Central England Temperature Series",
	journal = "Journal of Climate",
	year = "2022",
	publisher = "American Meteorological Society",
	address = "Boston MA, USA",
	volume = "35",
	number = "19",
	doi = "10.1175/JCLI-D-21-0489.1",
	pages=      "6329 - 6342",
	url = "https://journals.ametsoc.org/view/journals/clim/35/19/JCLI-D-21-0489.1.xml"
}

@article{SuarezSierra2023,
	author    = {Su{\'a}rez{-}Sierra, Biviana Marcela and Coen, Arrigo and Taimal, Carlos Alberto},
	title     = {Genetic algorithm with a Bayesian approach for multiple change-point detection in time series of counting exceedances for specific thresholds},
	journal   = {Journal of the Korean Statistical Society},
	volume    = {52},
	number    = {4},
	pages     = {982--1024},
	year      = {2023},
	doi       = {10.1007/s42952-023-00227-2},
	url       = {https://doi.org/10.1007/s42952-023-00227-2},
	issn      = {2005-2863},
	abstract  = {Although the applications of Non-Homogeneous Poisson Processes (NHPP) to model and study the threshold overshoots of interest in different time series of measurements have proven to provide good results, they needed to be complemented with an efficient and automatic diagnostic technique to establish the location of the change-points, which, when taken into account, make the estimated model fit poorly in regards of the information contained in the real one. Because of this, a new method is proposed to solve the segmentation uncertainty of the time series of measurements, where the generating distribution of exceedances of a specific threshold is the focus of investigation. One of the great contributions of the present algorithm is that all the days that trespassed are candidates to be a change-point, so all the possible configurations of overflow days under the heuristics of a genetic algorithm are the possible chromosomes, which will unite to produce new solutions. Also, such methods will be guarantee to non-local and the best possible one solution, reducing wasted machine time evaluating the least likely chromosomes to be a feasible solution. The analytical evaluation technique will be by means of the Minimum Description Length (MDL) as the objective function, which is the joint posterior distribution function of the parameters of the NHPP of each regime and the change-points that determines them and which account as well for the influence of the presence of said times. Thus, one of the practical implications of the present work comes in terms of overcoming the need of modeling the time series of measurements, where the distributions of exceedances of certain thresholds, or where the counting of certain events involving abrupt changes, is the main focus with applications in phenomena such as climate change, information security and epidemiology, to name a few.}
}


